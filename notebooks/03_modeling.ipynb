{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qXszINDMVmcQ"
   },
   "source": [
    "## **0. Environment Setup**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "j6XqhFlelO5n"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "6_x3uBA7USkW"
   },
   "outputs": [],
   "source": [
    "# Dataset path\n",
    "DATASET_PATH = \"../data/\"  # contains folders 1,2,...,8\n",
    "\n",
    "# !pip install mediapipe opencv-python scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oeWd1TEelfEM"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1766785325.278750 12199637 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 90.5), renderer: Apple M1 Pro\n",
      "I0000 00:00:1766785325.290223 12199637 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 90.5), renderer: Apple M1 Pro\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1766785325.293855 12201609 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import pickle\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "# from google.colab.patches import cv2_imshow\n",
    "import random\n",
    "\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(\n",
    "    static_image_mode=False,\n",
    "    model_complexity=1,\n",
    "    enable_segmentation=False,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "\n",
    "# Add FaceMesh for emotions\n",
    "mp_face = mp.solutions.face_mesh\n",
    "face_mesh = mp_face.FaceMesh(\n",
    "    static_image_mode=False,\n",
    "    max_num_faces=1,\n",
    "    refine_landmarks=True,  # includes landmarks for eyes and lips\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "tQsPl9eZWGD7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1766785325.304293 12201610 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "def extract_face_features(rgb_frame):\n",
    "    \"\"\"\n",
    "    Extract facial emotion features using FaceMesh.\n",
    "    \"\"\"\n",
    "    result = face_mesh.process(rgb_frame)\n",
    "    N_FACE_FEATURES = 20\n",
    "    \n",
    "    if not result.multi_face_landmarks:\n",
    "        return np.zeros(N_FACE_FEATURES)\n",
    "    \n",
    "    face_landmarks = result.multi_face_landmarks[0].landmark\n",
    "    nose = face_landmarks[1]\n",
    "    left_eye = face_landmarks[33]\n",
    "    right_eye = face_landmarks[263]\n",
    "    eye_dist = max(np.sqrt((left_eye.x - right_eye.x)**2 + (left_eye.y - right_eye.y)**2), 0.01)\n",
    "    \n",
    "    features = []\n",
    "\n",
    "    features.extend([(face_landmarks[159].y - face_landmarks[145].y) / eye_dist,\n",
    "                     (face_landmarks[386].y - face_landmarks[374].y) / eye_dist])\n",
    "\n",
    "    features.extend([(face_landmarks[105].y - face_landmarks[159].y) / eye_dist,\n",
    "                     (face_landmarks[334].y - face_landmarks[386].y) / eye_dist])\n",
    "\n",
    "    mouth_open = (face_landmarks[14].y - face_landmarks[13].y) / eye_dist\n",
    "    mouth_width = (face_landmarks[308].x - face_landmarks[78].x) / eye_dist\n",
    "    features.extend([mouth_open, mouth_width])\n",
    "    mouth_center_y = (face_landmarks[13].y + face_landmarks[14].y) / 2\n",
    "    features.extend([(mouth_center_y - face_landmarks[78].y) / eye_dist,\n",
    "                     (mouth_center_y - face_landmarks[308].y) / eye_dist])\n",
    "\n",
    "    features.extend([(left_eye.y - right_eye.y) / eye_dist,\n",
    "                     (nose.x - (left_eye.x + right_eye.x) / 2) / eye_dist])\n",
    "    features.append((face_landmarks[13].y - nose.y) / eye_dist)\n",
    "    for idx in [33, 263, 61, 291, 199, 175, 152, 10, 234]:\n",
    "        features.append((face_landmarks[idx].x - nose.x) / eye_dist)\n",
    "    \n",
    "    return np.array(features[:N_FACE_FEATURES])\n",
    "\n",
    "\n",
    "def extract_pose_vector(frame):\n",
    "    \"\"\"\n",
    "    Takes BGR frame, returns pose + face + hand features vector.\n",
    "    \"\"\"\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    result = pose.process(rgb)\n",
    "\n",
    "    if not result.pose_landmarks:\n",
    "        return None\n",
    "\n",
    "    landmarks = result.pose_landmarks.landmark\n",
    "    \n",
    "    LEFT_SHOULDER, RIGHT_SHOULDER = 11, 12\n",
    "    LEFT_HIP, RIGHT_HIP = 23, 24\n",
    "    LEFT_WRIST, RIGHT_WRIST = 15, 16\n",
    "    \n",
    "    center_x = (landmarks[LEFT_SHOULDER].x + landmarks[RIGHT_SHOULDER].x + \n",
    "                landmarks[LEFT_HIP].x + landmarks[RIGHT_HIP].x) / 4\n",
    "    center_y = (landmarks[LEFT_SHOULDER].y + landmarks[RIGHT_SHOULDER].y + \n",
    "                landmarks[LEFT_HIP].y + landmarks[RIGHT_HIP].y) / 4\n",
    "    \n",
    "    shoulder_dist = max(np.sqrt(\n",
    "        (landmarks[LEFT_SHOULDER].x - landmarks[RIGHT_SHOULDER].x) ** 2 +\n",
    "        (landmarks[LEFT_SHOULDER].y - landmarks[RIGHT_SHOULDER].y) ** 2\n",
    "    ), 0.01)\n",
    "\n",
    "    vec = []\n",
    "    for lm in landmarks:\n",
    "        norm_x = (lm.x - center_x) / shoulder_dist\n",
    "        norm_y = (lm.y - center_y) / shoulder_dist\n",
    "        vec.extend([norm_x, norm_y, lm.visibility])\n",
    "\n",
    "    # Hand visibility and position features\n",
    "    left_wrist_vis = landmarks[LEFT_WRIST].visibility\n",
    "    right_wrist_vis = landmarks[RIGHT_WRIST].visibility\n",
    "    left_hand_visible = 1.0 if left_wrist_vis > 0.5 else 0.0\n",
    "    right_hand_visible = 1.0 if right_wrist_vis > 0.5 else 0.0\n",
    "    any_hand_visible = 1.0 if (left_wrist_vis > 0.5 or right_wrist_vis > 0.5) else 0.0\n",
    "    left_wrist_above_shoulder = 1.0 if landmarks[LEFT_WRIST].y < landmarks[LEFT_SHOULDER].y else 0.0\n",
    "    right_wrist_above_shoulder = 1.0 if landmarks[RIGHT_WRIST].y < landmarks[RIGHT_SHOULDER].y else 0.0\n",
    "    left_hand_dist = np.sqrt((landmarks[LEFT_WRIST].x - center_x)**2 + \n",
    "                             (landmarks[LEFT_WRIST].y - center_y)**2) / shoulder_dist\n",
    "    right_hand_dist = np.sqrt((landmarks[RIGHT_WRIST].x - center_x)**2 + \n",
    "                              (landmarks[RIGHT_WRIST].y - center_y)**2) / shoulder_dist\n",
    "    \n",
    "    vec.extend([left_hand_visible, right_hand_visible, any_hand_visible,\n",
    "                left_wrist_above_shoulder, right_wrist_above_shoulder,\n",
    "                left_hand_dist, right_hand_dist])\n",
    "\n",
    "\n",
    "    face_features = extract_face_features(rgb)\n",
    "    vec.extend(face_features)\n",
    "\n",
    "    return np.array(vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "zQWyIbkOWVXF"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1766785325.381143 12201604 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1766785325.400707 12201607 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET_PATH content:\n",
      "\u001b[34m1\u001b[m\u001b[m         \u001b[34m3\u001b[m\u001b[m         \u001b[34m5\u001b[m\u001b[m         \u001b[34m7\u001b[m\u001b[m         README.md\n",
      "\u001b[34m2\u001b[m\u001b[m         \u001b[34m4\u001b[m\u001b[m         \u001b[34m6\u001b[m\u001b[m         \u001b[34m8\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "print(\"DATASET_PATH content:\")\n",
    "!ls \"$DATASET_PATH\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Create augmentations\n",
    "augmentations = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),      # Horizontal flip\n",
    "    transforms.RandomRotation(degrees=15),       # Rotate +- 15 deg\n",
    "    transforms.ColorJitter(brightness=0.2),      # Jitter brightness\n",
    "    transforms.RandomResizedCrop(size=(224, 224), scale=(0.7, 1.0)) # Crop and resize\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "m1W19UGgW4Fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Class 1\n",
      "    Video: 1_1.mov\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1766785325.819780 12201603 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Video: 1_2.mov\n",
      "    Video: 1_3.mov\n",
      "    Video: 1_4.MOV\n",
      "    Video: 1_5.mp4\n",
      "→ Class 2\n",
      "    Video: 2_1.mov\n",
      "    Video: 2_2.mov\n",
      "    Video: 2_3.mov\n",
      "    Video: 2_4.MOV\n",
      "    Video: 2_5.mp4\n",
      "→ Class 3\n",
      "    Video: 3_1.mov\n",
      "    Video: 3_2.mov\n",
      "    Video: 3_3.mov\n",
      "    Video: 3_4.MOV\n",
      "    Video: 3_5.mp4\n",
      "→ Class 4\n",
      "    Video: 4_1.mov\n",
      "    Video: 4_2.mov\n",
      "    Video: 4_3.mov\n",
      "    Video: 4_4.MOV\n",
      "    Video: 4_5.mp4\n",
      "→ Class 5\n",
      "    Video: 5_1.mov\n",
      "    Video: 5_2.mov\n",
      "    Video: 5_3.mov\n",
      "    Video: 5_4.MOV\n",
      "    Video: 5_5.mp4\n",
      "→ Class 6\n",
      "    Video: 6_1.mov\n",
      "    Video: 6_2.mov\n",
      "    Video: 6_3.mov\n",
      "    Video: 6_4.MOV\n",
      "    Video: 6_5.mp4\n",
      "→ Class 7\n",
      "    Video: 7_1.mov\n",
      "    Video: 7_2.mov\n",
      "    Video: 7_3.mov\n",
      "    Video: 7_4.mov\n",
      "    Video: 7_5.mp4\n",
      "→ Class 8\n",
      "    Video: 8_1.mov\n",
      "    Video: 8_2.mov\n",
      "    Video: 8_3.mov\n",
      "    Video: 8_4.mov\n",
      "    Video: 8_5.mov\n",
      "Total poses: 4259\n",
      "Classes: ['1' '2' '3' '4' '5' '6' '7' '8']\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "video_ext = (\".mp4\", \".avi\", \".mov\", \".mkv\")\n",
    "\n",
    "for class_name in sorted(os.listdir(DATASET_PATH)):\n",
    "    class_path = os.path.join(DATASET_PATH, class_name)\n",
    "    if not os.path.isdir(class_path):\n",
    "        continue\n",
    "\n",
    "    print(f\"→ Class {class_name}\")\n",
    "\n",
    "    for fname in sorted(os.listdir(class_path)):\n",
    "        if not fname.lower().endswith(video_ext):\n",
    "            continue\n",
    "\n",
    "        video_path = os.path.join(class_path, fname)\n",
    "        print(f\"    Video: {fname}\")\n",
    "\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frame_id = 0\n",
    "\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            # 1. Take every 2nd frame\n",
    "            if frame_id % 2 == 0:\n",
    "                \n",
    "                # --- AUGMENTATION BLOCK ---\n",
    "                # Copy frame for augmentation\n",
    "                aug_frame = frame.copy()\n",
    "\n",
    "\n",
    "                # Prevents model from bias to one side\n",
    "                if random.random() > 0.5:\n",
    "                    aug_frame = cv2.flip(aug_frame, 1)\n",
    "\n",
    "\n",
    "                angle = random.uniform(-15, 15)\n",
    "                h, w = aug_frame.shape[:2]\n",
    "                M = cv2.getRotationMatrix2D((w // 2, h // 2), angle, 1.0)\n",
    "                aug_frame = cv2.warpAffine(aug_frame, M, (w, h))\n",
    "\n",
    "\n",
    "                brightness = random.uniform(0.7, 1.3)\n",
    "                aug_frame = cv2.convertScaleAbs(aug_frame, alpha=brightness, beta=0)\n",
    "                \n",
    "                # --- VECTOR EXTRACTION ---\n",
    "                # Process augmented frame\n",
    "                vec = extract_pose_vector(aug_frame)\n",
    "                \n",
    "                if vec is not None:\n",
    "                    X.append(vec)\n",
    "                    y.append(class_name)\n",
    "\n",
    "            frame_id += 1\n",
    "\n",
    "        cap.release()\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "print(\"Total poses:\", len(X))\n",
    "print(\"Classes:\", np.unique(y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "sAWvJ3A4ZjfZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA: 126 -> 24 components (98% variance)\n"
     ]
    }
   ],
   "source": [
    "if len(np.unique(y)) < 2:\n",
    "    raise ValueError(f\"Only one class found: {np.unique(y)}. Check folder structure and data.\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# PCA: keep 98% variance for better quality\n",
    "pca = PCA(n_components=0.98)\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "print(f\"PCA: {X_train_scaled.shape[1]} -> {X_train_pca.shape[1]} components (98% variance)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "VLSoC9aulNDY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quality report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.96      0.88      0.92       146\n",
      "           2       0.97      0.97      0.97        90\n",
      "           3       0.96      0.96      0.96       129\n",
      "           4       1.00      0.92      0.96        61\n",
      "           5       0.96      0.97      0.96       117\n",
      "           6       0.92      0.97      0.95       137\n",
      "           7       1.00      0.96      0.98       134\n",
      "           8       0.72      1.00      0.84        38\n",
      "\n",
      "    accuracy                           0.95       852\n",
      "   macro avg       0.94      0.95      0.94       852\n",
      "weighted avg       0.95      0.95      0.95       852\n",
      "\n",
      "Confusion matrix:\n",
      "[[129   2   0   0   1   8   0   6]\n",
      " [  0  87   0   0   0   1   0   2]\n",
      " [  0   0 124   0   3   2   0   0]\n",
      " [  0   0   0  56   0   0   0   5]\n",
      " [  1   0   3   0 113   0   0   0]\n",
      " [  3   0   0   0   0 133   0   1]\n",
      " [  1   1   2   0   1   0 128   1]\n",
      " [  0   0   0   0   0   0   0  38]]\n"
     ]
    }
   ],
   "source": [
    "# Best parameters found: C=200, gamma=0.05\n",
    "clf = SVC(C=200, gamma=0.05, kernel='rbf', probability=True)\n",
    "clf.fit(X_train_pca, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test_pca)\n",
    "print(\"Quality report:\\n\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "Ld7v7uWCtghq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at: ../models/model.pkl\n"
     ]
    }
   ],
   "source": [
    "model = {\n",
    "    \"clf\": clf,\n",
    "    \"scaler\": scaler,\n",
    "    \"pca\": pca,\n",
    "    \"classes\": sorted(list(np.unique(y)))\n",
    "}\n",
    "\n",
    "model_path = \"../models/model.pkl\"\n",
    "with open(model_path, \"wb\") as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "print(\"Model saved at:\", model_path)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
