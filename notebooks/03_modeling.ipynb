{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qXszINDMVmcQ"
   },
   "source": [
    "## **0. Environment Setup**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "6_x3uBA7USkW"
   },
   "outputs": [],
   "source": [
    "DATASET_PATH = \"../data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "oeWd1TEelfEM"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1766794324.401228 12365986 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 90.5), renderer: Apple M1 Pro\n",
      "I0000 00:00:1766794324.410465 12365986 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 90.5), renderer: Apple M1 Pro\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pickle\n",
    "import sys\n",
    "import random\n",
    "\n",
    "# Add project root to path to import from src\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from src.preprocessing import extract_pose_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "zQWyIbkOWVXF"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1766794324.431387 12389385 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1766794324.447659 12389389 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1766794324.495865 12389376 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1766794324.509886 12389381 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET_PATH content:\n",
      "\u001b[34m1\u001b[m\u001b[m         \u001b[34m3\u001b[m\u001b[m         \u001b[34m5\u001b[m\u001b[m         \u001b[34m7\u001b[m\u001b[m         README.md\n",
      "\u001b[34m2\u001b[m\u001b[m         \u001b[34m4\u001b[m\u001b[m         \u001b[34m6\u001b[m\u001b[m         \u001b[34m8\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "print(\"DATASET_PATH content:\")\n",
    "!ls \"$DATASET_PATH\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Create augmentations\n",
    "augmentations = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),      # Horizontal flip\n",
    "    transforms.RandomRotation(degrees=15),       # Rotate +- 15 deg\n",
    "    transforms.ColorJitter(brightness=0.2),      # Jitter brightness\n",
    "    transforms.RandomResizedCrop(size=(224, 224), scale=(0.7, 1.0)) # Crop and resize\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "m1W19UGgW4Fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Class 1\n",
      "    Video: 1_1.mov\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1766794324.846409 12389379 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Video: 1_2.mov\n",
      "    Video: 1_3.mov\n",
      "    Video: 1_4.MOV\n",
      "    Video: 1_5.mp4\n",
      "→ Class 2\n",
      "    Video: 2_1.mov\n",
      "    Video: 2_2.mov\n",
      "    Video: 2_3.mov\n",
      "    Video: 2_4.MOV\n",
      "    Video: 2_5.mp4\n",
      "→ Class 3\n",
      "    Video: 3_1.mov\n",
      "    Video: 3_2.mov\n",
      "    Video: 3_3.mov\n",
      "    Video: 3_4.MOV\n",
      "    Video: 3_5.mp4\n",
      "→ Class 4\n",
      "    Video: 4_1.mov\n",
      "    Video: 4_2.mov\n",
      "    Video: 4_3.mov\n",
      "    Video: 4_4.MOV\n",
      "    Video: 4_5.mp4\n",
      "→ Class 5\n",
      "    Video: 5_1.mov\n",
      "    Video: 5_2.mov\n",
      "    Video: 5_3.mov\n",
      "    Video: 5_4.MOV\n",
      "    Video: 5_5.mp4\n",
      "→ Class 6\n",
      "    Video: 6_1.mov\n",
      "    Video: 6_2.mov\n",
      "    Video: 6_3.mov\n",
      "    Video: 6_4.MOV\n",
      "    Video: 6_5.mp4\n",
      "→ Class 7\n",
      "    Video: 7_1.mov\n",
      "    Video: 7_2.mov\n",
      "    Video: 7_3.mov\n",
      "    Video: 7_4.mov\n",
      "    Video: 7_5.mp4\n",
      "→ Class 8\n",
      "    Video: 8_1.mov\n",
      "    Video: 8_2.mov\n",
      "    Video: 8_3.mov\n",
      "    Video: 8_4.mov\n",
      "    Video: 8_5.mov\n",
      "Total poses: 4251\n",
      "Classes: ['1' '2' '3' '4' '5' '6' '7' '8']\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "video_ext = (\".mp4\", \".avi\", \".mov\", \".mkv\")\n",
    "\n",
    "for class_name in sorted(os.listdir(DATASET_PATH)):\n",
    "    class_path = os.path.join(DATASET_PATH, class_name)\n",
    "    if not os.path.isdir(class_path):\n",
    "        continue\n",
    "\n",
    "    print(f\"→ Class {class_name}\")\n",
    "\n",
    "    for fname in sorted(os.listdir(class_path)):\n",
    "        if not fname.lower().endswith(video_ext):\n",
    "            continue\n",
    "\n",
    "        video_path = os.path.join(class_path, fname)\n",
    "        print(f\"    Video: {fname}\")\n",
    "\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frame_id = 0\n",
    "\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            # 1. Take every 2nd frame\n",
    "            if frame_id % 2 == 0:\n",
    "                \n",
    "                # --- AUGMENTATION BLOCK ---\n",
    "                # Copy frame for augmentation\n",
    "                aug_frame = frame.copy()\n",
    "\n",
    "\n",
    "                # Prevents model from bias to one side\n",
    "                if random.random() > 0.5:\n",
    "                    aug_frame = cv2.flip(aug_frame, 1)\n",
    "\n",
    "\n",
    "                angle = random.uniform(-15, 15)\n",
    "                h, w = aug_frame.shape[:2]\n",
    "                M = cv2.getRotationMatrix2D((w // 2, h // 2), angle, 1.0)\n",
    "                aug_frame = cv2.warpAffine(aug_frame, M, (w, h))\n",
    "\n",
    "\n",
    "                brightness = random.uniform(0.7, 1.3)\n",
    "                aug_frame = cv2.convertScaleAbs(aug_frame, alpha=brightness, beta=0)\n",
    "                \n",
    "                # --- VECTOR EXTRACTION ---\n",
    "                # Process augmented frame\n",
    "                vec = extract_pose_vector(aug_frame)\n",
    "                \n",
    "                if vec is not None:\n",
    "                    X.append(vec)\n",
    "                    y.append(class_name)\n",
    "\n",
    "            frame_id += 1\n",
    "\n",
    "        cap.release()\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "print(\"Total poses:\", len(X))\n",
    "print(\"Classes:\", np.unique(y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "sAWvJ3A4ZjfZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA: 126 -> 24 components (98% variance)\n"
     ]
    }
   ],
   "source": [
    "if len(np.unique(y)) < 2:\n",
    "    raise ValueError(f\"Only one class found: {np.unique(y)}. Check folder structure and data.\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# PCA: keep 98% variance for better quality\n",
    "pca = PCA(n_components=0.98)\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "print(f\"PCA: {X_train_scaled.shape[1]} -> {X_train_pca.shape[1]} components (98% variance)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "VLSoC9aulNDY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quality report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.97      0.95      0.96       146\n",
      "           2       1.00      0.98      0.99        90\n",
      "           3       0.97      0.92      0.94       129\n",
      "           4       1.00      0.95      0.97        60\n",
      "           5       0.96      0.96      0.96       117\n",
      "           6       0.94      0.98      0.96       137\n",
      "           7       0.96      0.98      0.97       134\n",
      "           8       0.86      1.00      0.93        38\n",
      "\n",
      "    accuracy                           0.96       851\n",
      "   macro avg       0.96      0.96      0.96       851\n",
      "weighted avg       0.96      0.96      0.96       851\n",
      "\n",
      "Confusion matrix:\n",
      "[[138   0   2   0   0   5   0   1]\n",
      " [  0  88   0   0   0   1   0   1]\n",
      " [  0   0 119   0   5   1   4   0]\n",
      " [  0   0   0  57   0   0   0   3]\n",
      " [  1   0   2   0 112   1   1   0]\n",
      " [  3   0   0   0   0 134   0   0]\n",
      " [  1   0   0   0   0   1 131   1]\n",
      " [  0   0   0   0   0   0   0  38]]\n"
     ]
    }
   ],
   "source": [
    "# Best parameters found: C=200, gamma=0.05\n",
    "clf = SVC(C=200, gamma=0.05, kernel='rbf', probability=True)\n",
    "clf.fit(X_train_pca, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test_pca)\n",
    "print(\"Quality report:\\n\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Ld7v7uWCtghq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at: ../models/model.pkl\n"
     ]
    }
   ],
   "source": [
    "model = {\n",
    "    \"clf\": clf,\n",
    "    \"scaler\": scaler,\n",
    "    \"pca\": pca,\n",
    "    \"classes\": sorted(list(np.unique(y)))\n",
    "}\n",
    "\n",
    "model_path = \"../models/model.pkl\"\n",
    "with open(model_path, \"wb\") as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "print(\"Model saved at:\", model_path)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
