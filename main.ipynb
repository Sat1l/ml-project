{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXszINDMVmcQ"
      },
      "source": [
        "## **0. Подготовка окружения**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "j6XqhFlelO5n"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "6_x3uBA7USkW"
      },
      "outputs": [],
      "source": [
        "# Путь к папке с данными\n",
        "DATASET_PATH = \"./data\"  # внутри: папки 1,2,...,8\n",
        "\n",
        "# !pip install mediapipe opencv-python scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "oeWd1TEelfEM"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1766780420.681324 12037841 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 90.5), renderer: Apple M1 Pro\n",
            "I0000 00:00:1766780420.697587 12037841 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 90.5), renderer: Apple M1 Pro\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import mediapipe as mp\n",
        "import pickle\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "# from google.colab.patches import cv2_imshow\n",
        "import random\n",
        "\n",
        "mp_pose = mp.solutions.pose\n",
        "pose = mp_pose.Pose(\n",
        "    static_image_mode=False,\n",
        "    model_complexity=1,\n",
        "    enable_segmentation=False,\n",
        "    min_detection_confidence=0.5,\n",
        "    min_tracking_confidence=0.5\n",
        ")\n",
        "\n",
        "# Добавляем FaceMesh для эмоций\n",
        "mp_face = mp.solutions.face_mesh\n",
        "face_mesh = mp_face.FaceMesh(\n",
        "    static_image_mode=False,\n",
        "    max_num_faces=1,\n",
        "    refine_landmarks=True,  # включает landmarks для глаз и губ\n",
        "    min_detection_confidence=0.5,\n",
        "    min_tracking_confidence=0.5\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "tQsPl9eZWGD7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "W0000 00:00:1766780420.706244 12098312 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1766780420.720864 12098312 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
          ]
        }
      ],
      "source": [
        "def extract_face_features(rgb_frame):\n",
        "    \"\"\"\n",
        "    Извлекает признаки эмоций из лица используя FaceMesh.\n",
        "    \"\"\"\n",
        "    result = face_mesh.process(rgb_frame)\n",
        "    N_FACE_FEATURES = 20\n",
        "    \n",
        "    if not result.multi_face_landmarks:\n",
        "        return np.zeros(N_FACE_FEATURES)\n",
        "    \n",
        "    face_landmarks = result.multi_face_landmarks[0].landmark\n",
        "    nose = face_landmarks[1]\n",
        "    left_eye = face_landmarks[33]\n",
        "    right_eye = face_landmarks[263]\n",
        "    eye_dist = max(np.sqrt((left_eye.x - right_eye.x)**2 + (left_eye.y - right_eye.y)**2), 0.01)\n",
        "    \n",
        "    features = []\n",
        "    # Открытость глаз\n",
        "    features.extend([(face_landmarks[159].y - face_landmarks[145].y) / eye_dist,\n",
        "                     (face_landmarks[386].y - face_landmarks[374].y) / eye_dist])\n",
        "    # Положение бровей\n",
        "    features.extend([(face_landmarks[105].y - face_landmarks[159].y) / eye_dist,\n",
        "                     (face_landmarks[334].y - face_landmarks[386].y) / eye_dist])\n",
        "    # Рот\n",
        "    mouth_open = (face_landmarks[14].y - face_landmarks[13].y) / eye_dist\n",
        "    mouth_width = (face_landmarks[308].x - face_landmarks[78].x) / eye_dist\n",
        "    features.extend([mouth_open, mouth_width])\n",
        "    mouth_center_y = (face_landmarks[13].y + face_landmarks[14].y) / 2\n",
        "    features.extend([(mouth_center_y - face_landmarks[78].y) / eye_dist,\n",
        "                     (mouth_center_y - face_landmarks[308].y) / eye_dist])\n",
        "    # Голова\n",
        "    features.extend([(left_eye.y - right_eye.y) / eye_dist,\n",
        "                     (nose.x - (left_eye.x + right_eye.x) / 2) / eye_dist])\n",
        "    features.append((face_landmarks[13].y - nose.y) / eye_dist)\n",
        "    for idx in [33, 263, 61, 291, 199, 175, 152, 10, 234]:\n",
        "        features.append((face_landmarks[idx].x - nose.x) / eye_dist)\n",
        "    \n",
        "    return np.array(features[:N_FACE_FEATURES])\n",
        "\n",
        "\n",
        "def extract_pose_vector(frame):\n",
        "    \"\"\"\n",
        "    Принимает BGR-кадр, возвращает вектор позы + лица + признаки рук.\n",
        "    \"\"\"\n",
        "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    result = pose.process(rgb)\n",
        "\n",
        "    if not result.pose_landmarks:\n",
        "        return None\n",
        "\n",
        "    landmarks = result.pose_landmarks.landmark\n",
        "    \n",
        "    LEFT_SHOULDER, RIGHT_SHOULDER = 11, 12\n",
        "    LEFT_HIP, RIGHT_HIP = 23, 24\n",
        "    LEFT_WRIST, RIGHT_WRIST = 15, 16\n",
        "    \n",
        "    center_x = (landmarks[LEFT_SHOULDER].x + landmarks[RIGHT_SHOULDER].x + \n",
        "                landmarks[LEFT_HIP].x + landmarks[RIGHT_HIP].x) / 4\n",
        "    center_y = (landmarks[LEFT_SHOULDER].y + landmarks[RIGHT_SHOULDER].y + \n",
        "                landmarks[LEFT_HIP].y + landmarks[RIGHT_HIP].y) / 4\n",
        "    \n",
        "    shoulder_dist = max(np.sqrt(\n",
        "        (landmarks[LEFT_SHOULDER].x - landmarks[RIGHT_SHOULDER].x) ** 2 +\n",
        "        (landmarks[LEFT_SHOULDER].y - landmarks[RIGHT_SHOULDER].y) ** 2\n",
        "    ), 0.01)\n",
        "\n",
        "    vec = []\n",
        "    for lm in landmarks:\n",
        "        norm_x = (lm.x - center_x) / shoulder_dist\n",
        "        norm_y = (lm.y - center_y) / shoulder_dist\n",
        "        vec.extend([norm_x, norm_y, lm.visibility])\n",
        "\n",
        "    # Явные признаки видимости и положения рук\n",
        "    left_wrist_vis = landmarks[LEFT_WRIST].visibility\n",
        "    right_wrist_vis = landmarks[RIGHT_WRIST].visibility\n",
        "    left_hand_visible = 1.0 if left_wrist_vis > 0.5 else 0.0\n",
        "    right_hand_visible = 1.0 if right_wrist_vis > 0.5 else 0.0\n",
        "    any_hand_visible = 1.0 if (left_wrist_vis > 0.5 or right_wrist_vis > 0.5) else 0.0\n",
        "    left_wrist_above_shoulder = 1.0 if landmarks[LEFT_WRIST].y < landmarks[LEFT_SHOULDER].y else 0.0\n",
        "    right_wrist_above_shoulder = 1.0 if landmarks[RIGHT_WRIST].y < landmarks[RIGHT_SHOULDER].y else 0.0\n",
        "    left_hand_dist = np.sqrt((landmarks[LEFT_WRIST].x - center_x)**2 + \n",
        "                             (landmarks[LEFT_WRIST].y - center_y)**2) / shoulder_dist\n",
        "    right_hand_dist = np.sqrt((landmarks[RIGHT_WRIST].x - center_x)**2 + \n",
        "                              (landmarks[RIGHT_WRIST].y - center_y)**2) / shoulder_dist\n",
        "    \n",
        "    vec.extend([left_hand_visible, right_hand_visible, any_hand_visible,\n",
        "                left_wrist_above_shoulder, right_wrist_above_shoulder,\n",
        "                left_hand_dist, right_hand_dist])\n",
        "\n",
        "    # Признаки лица\n",
        "    face_features = extract_face_features(rgb)\n",
        "    vec.extend(face_features)\n",
        "\n",
        "    return np.array(vec)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "zQWyIbkOWVXF"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "W0000 00:00:1766780420.857357 12098301 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1766780420.892164 12098305 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Содержимое DATASET_PATH:\n",
            "\u001b[34m1\u001b[m\u001b[m         \u001b[34m3\u001b[m\u001b[m         \u001b[34m5\u001b[m\u001b[m         \u001b[34m7\u001b[m\u001b[m         model.pkl\n",
            "\u001b[34m2\u001b[m\u001b[m         \u001b[34m4\u001b[m\u001b[m         \u001b[34m6\u001b[m\u001b[m         \u001b[34m8\u001b[m\u001b[m\n"
          ]
        }
      ],
      "source": [
        "print(\"Содержимое DATASET_PATH:\")\n",
        "!ls \"$DATASET_PATH\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "# Создаем трансформер\n",
        "augmentations = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(p=0.5),      # Отражаем слева направо\n",
        "    transforms.RandomRotation(degrees=15),       # Крутим на +- 15 градусов\n",
        "    transforms.ColorJitter(brightness=0.2),      # Меняем яркость\n",
        "    transforms.RandomResizedCrop(size=(224, 224), scale=(0.7, 1.0)) # ГЛАВНОЕ: зум и обрезка\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "m1W19UGgW4Fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "→ Класс 1\n",
            "    Видео: 1_1.mov\n",
            "    Видео: 1_2.mov\n",
            "    Видео: 1_3.mov\n",
            "    Видео: 1_4.MOV\n",
            "    Видео: 1_5.mp4\n",
            "→ Класс 2\n",
            "    Видео: 2_1.mov\n",
            "    Видео: 2_2.mov\n",
            "    Видео: 2_3.mov\n",
            "    Видео: 2_4.MOV\n",
            "    Видео: 2_5.mp4\n",
            "→ Класс 3\n",
            "    Видео: 3_1.mov\n",
            "    Видео: 3_2.mov\n",
            "    Видео: 3_3.mov\n",
            "    Видео: 3_4.MOV\n",
            "    Видео: 3_5.mp4\n",
            "→ Класс 4\n",
            "    Видео: 4_1.mov\n",
            "    Видео: 4_2.mov\n",
            "    Видео: 4_3.mov\n",
            "    Видео: 4_4.MOV\n",
            "    Видео: 4_5.mp4\n",
            "→ Класс 5\n",
            "    Видео: 5_1.mov\n",
            "    Видео: 5_2.mov\n",
            "    Видео: 5_3.mov\n",
            "    Видео: 5_4.MOV\n",
            "    Видео: 5_5.mp4\n",
            "→ Класс 6\n",
            "    Видео: 6_1.mov\n",
            "    Видео: 6_2.mov\n",
            "    Видео: 6_3.mov\n",
            "    Видео: 6_4.MOV\n",
            "    Видео: 6_5.mp4\n",
            "→ Класс 7\n",
            "    Видео: 7_1.mov\n",
            "    Видео: 7_2.mov\n",
            "    Видео: 7_3.mov\n",
            "    Видео: 7_4.MOV\n",
            "    Видео: 7_5.mp4\n",
            "→ Класс 8\n",
            "    Видео: 8_1.mov\n",
            "    Видео: 8_2.mov\n",
            "    Видео: 8_3.mov\n",
            "    Видео: 8_4.MOV\n",
            "    Видео: 8_5.mp4\n",
            "Всего поз: 5000\n",
            "Классы: ['1' '2' '3' '4' '5' '6' '7' '8']\n"
          ]
        }
      ],
      "source": [
        "X = []\n",
        "y = []\n",
        "\n",
        "video_ext = (\".mp4\", \".avi\", \".mov\", \".mkv\")\n",
        "\n",
        "for class_name in sorted(os.listdir(DATASET_PATH)):\n",
        "    class_path = os.path.join(DATASET_PATH, class_name)\n",
        "    if not os.path.isdir(class_path):\n",
        "        continue\n",
        "\n",
        "    print(f\"→ Класс {class_name}\")\n",
        "\n",
        "    for fname in sorted(os.listdir(class_path)):\n",
        "        if not fname.lower().endswith(video_ext):\n",
        "            continue\n",
        "\n",
        "        video_path = os.path.join(class_path, fname)\n",
        "        print(f\"    Видео: {fname}\")\n",
        "\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        frame_id = 0\n",
        "\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            # 1. ТЕПЕРЬ БЕРЕМ КАЖДЫЙ 2-й КАДР (в 5 раз больше данных, чем было)\n",
        "            if frame_id % 2 == 0:\n",
        "                \n",
        "                # --- БЛОК АУГМЕНТАЦИИ ---\n",
        "                # Создаем копию кадра, чтобы не портить оригинал для следующих шагов\n",
        "                aug_frame = frame.copy()\n",
        "\n",
        "                # А. Рандомное отражение (слева-направо)\n",
        "                # Это самое важное, чтобы модель не привыкала, что рука в одном углу\n",
        "                if random.random() > 0.5:\n",
        "                    aug_frame = cv2.flip(aug_frame, 1)\n",
        "\n",
        "                # Б. Рандомный поворот (от -15 до 15 градусов)\n",
        "                angle = random.uniform(-15, 15)\n",
        "                h, w = aug_frame.shape[:2]\n",
        "                M = cv2.getRotationMatrix2D((w // 2, h // 2), angle, 1.0)\n",
        "                aug_frame = cv2.warpAffine(aug_frame, M, (w, h))\n",
        "\n",
        "                # В. Рандомное изменение яркости (чтобы не привязываться к свету)\n",
        "                brightness = random.uniform(0.7, 1.3)\n",
        "                aug_frame = cv2.convertScaleAbs(aug_frame, alpha=brightness, beta=0)\n",
        "                \n",
        "                # --- ИЗВЛЕЧЕНИЕ ВЕКТОРА ---\n",
        "                # Передаем уже измененный кадр\n",
        "                vec = extract_pose_vector(aug_frame)\n",
        "                \n",
        "                if vec is not None:\n",
        "                    X.append(vec)\n",
        "                    y.append(class_name)\n",
        "\n",
        "            frame_id += 1\n",
        "\n",
        "        cap.release()\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "print(\"Всего поз:\", len(X))\n",
        "print(\"Классы:\", np.unique(y))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "sAWvJ3A4ZjfZ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PCA: 126 -> 24 компонент (98% дисперсии)\n"
          ]
        }
      ],
      "source": [
        "if len(np.unique(y)) < 2:\n",
        "    raise ValueError(f\"Нашли только один класс: {np.unique(y)}. Проверь структуру папок и данные.\")\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# PCA: сохраняем 98% дисперсии для лучшего качества\n",
        "pca = PCA(n_components=0.98)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "print(f\"PCA: {X_train_scaled.shape[1]} -> {X_train_pca.shape[1]} компонент (98% дисперсии)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "VLSoC9aulNDY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Отчёт по качеству:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.89      0.87      0.88       169\n",
            "           2       0.96      0.94      0.95       101\n",
            "           3       0.90      0.88      0.89       151\n",
            "           4       1.00      0.90      0.95        61\n",
            "           5       0.92      0.89      0.90       122\n",
            "           6       0.89      0.91      0.90       148\n",
            "           7       0.96      0.94      0.95       170\n",
            "           8       0.73      0.92      0.82        78\n",
            "\n",
            "    accuracy                           0.91      1000\n",
            "   macro avg       0.91      0.91      0.91      1000\n",
            "weighted avg       0.91      0.91      0.91      1000\n",
            "\n",
            "Матрица ошибок:\n",
            "[[147   1   5   0   0   8   0   8]\n",
            " [  4  95   0   0   0   0   0   2]\n",
            " [  3   1 133   0   5   2   3   4]\n",
            " [  0   0   0  55   0   0   0   6]\n",
            " [  3   1   8   0 108   0   1   1]\n",
            " [  7   0   0   0   2 135   2   2]\n",
            " [  0   0   1   0   2   4 160   3]\n",
            " [  2   1   0   0   0   2   1  72]]\n"
          ]
        }
      ],
      "source": [
        "# Лучшие параметры найдены экспериментально: C=200, gamma=0.05\n",
        "clf = SVC(C=200, gamma=0.05, kernel='rbf', probability=True)\n",
        "clf.fit(X_train_pca, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test_pca)\n",
        "print(\"Отчёт по качеству:\\n\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"Матрица ошибок:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "Ld7v7uWCtghq"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Модель сохранена в: ./data/model.pkl\n"
          ]
        }
      ],
      "source": [
        "model = {\n",
        "    \"clf\": clf,\n",
        "    \"scaler\": scaler,\n",
        "    \"pca\": pca,\n",
        "    \"classes\": sorted(list(np.unique(y)))\n",
        "}\n",
        "\n",
        "model_path = os.path.join(DATASET_PATH, \"model.pkl\")\n",
        "with open(model_path, \"wb\") as f:\n",
        "    pickle.dump(model, f)\n",
        "\n",
        "print(\"Модель сохранена в:\", model_path)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
